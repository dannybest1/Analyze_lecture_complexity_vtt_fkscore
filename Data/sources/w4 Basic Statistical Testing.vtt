WEBVTT

1
00:00:05.450 --> 00:00:08.040
In this lecture, we're
going to review some of

2
00:00:08.040 --> 00:00:10.605
the basic statistical
testing in Python.

3
00:00:10.605 --> 00:00:12.630
We're going to talk about
hypothesis testing,

4
00:00:12.630 --> 00:00:14.655
statistical
significance, and using

5
00:00:14.655 --> 00:00:17.565
SciPy to run
the student's t-test.

6
00:00:17.565 --> 00:00:19.515
We use statistics a lot

7
00:00:19.515 --> 00:00:21.240
in different ways
in data science,

8
00:00:21.240 --> 00:00:23.295
and in this lecture
I want to refresh

9
00:00:23.295 --> 00:00:25.500
your knowledge of
hypothesis testing,

10
00:00:25.500 --> 00:00:27.947
which is a core data
analysis activity

11
00:00:27.947 --> 00:00:29.910
behind experimentation.

12
00:00:29.910 --> 00:00:32.385
The goal of hypothesis testing

13
00:00:32.385 --> 00:00:34.410
is to determine if for instance

14
00:00:34.410 --> 00:00:36.450
the two different
conditions we have in

15
00:00:36.450 --> 00:00:39.450
an experiment have resulted
in different impacts.

16
00:00:39.450 --> 00:00:43.220
So let's import our usual
NumPy and pandas libraries,

17
00:00:43.220 --> 00:00:46.015
this is np and pandas as pd.

18
00:00:46.015 --> 00:00:48.965
Now, let's bring in some new
libraries from SciPy.

19
00:00:48.965 --> 00:00:52.220
So from SciPy I want
to import stats.

20
00:00:52.220 --> 00:00:55.100
Now SciPy is an
interesting collection

21
00:00:55.100 --> 00:00:56.660
of libraries for data science,

22
00:00:56.660 --> 00:01:00.005
and you'll use most or perhaps
all of these libraries.

23
00:01:00.005 --> 00:01:02.030
It includes NumPy and pandas,

24
00:01:02.030 --> 00:01:04.850
but also plotting libraries
such as Matplotlib and

25
00:01:04.850 --> 00:01:08.365
a number of other scientific
library functions as well.

26
00:01:08.365 --> 00:01:10.455
When we do hypothesis testing,

27
00:01:10.455 --> 00:01:12.800
we actually have
two statements of interest.

28
00:01:12.800 --> 00:01:14.584
The first is
our actual explanation,

29
00:01:14.584 --> 00:01:16.669
which we call the
alternative hypothesis,

30
00:01:16.669 --> 00:01:19.595
and the second is that
the explanation we have is

31
00:01:19.595 --> 00:01:23.035
not sufficient and we call
this the null hypothesis.

32
00:01:23.035 --> 00:01:26.030
Our actual testing method
is to determine

33
00:01:26.030 --> 00:01:29.150
whether the null hypothesis
is true or not.

34
00:01:29.150 --> 00:01:30.650
If we find that there is

35
00:01:30.650 --> 00:01:32.840
a difference between
groups then we can

36
00:01:32.840 --> 00:01:37.010
reject the null hypothesis and
we accept our alternative.

37
00:01:37.010 --> 00:01:38.645
So let's see an example of this.

38
00:01:38.645 --> 00:01:40.370
We're going to use
some grade data.

39
00:01:40.370 --> 00:01:42.230
So make a new DataFrame

40
00:01:42.230 --> 00:01:44.210
here and read _csv
(datasets/grades.csv).

41
00:01:44.210 --> 00:01:47.710
Let's look at the head of that.

42
00:01:47.710 --> 00:01:49.970
If we take a look
at the DataFrame

43
00:01:49.970 --> 00:01:52.220
inside we have
six different assignments.

44
00:01:52.220 --> 00:01:55.130
Let's look at some summary
statistics for this DataFrame.

45
00:01:55.130 --> 00:01:56.540
So I'm just going to
print out the number

46
00:01:56.540 --> 00:01:57.830
of rows and columns,

47
00:01:57.830 --> 00:01:59.390
and remember the DataFrame has

48
00:01:59.390 --> 00:02:03.580
a shape and we'll just
pass those as variables.

49
00:02:03.580 --> 00:02:06.005
All right, for the purpose
of this lecture let's

50
00:02:06.005 --> 00:02:08.780
segment this population
into two pieces.

51
00:02:08.780 --> 00:02:09.835
Let's say those who finish

52
00:02:09.835 --> 00:02:12.605
the first assignment by
the end of December 2015,

53
00:02:12.605 --> 00:02:14.255
we'll call them early finishers,

54
00:02:14.255 --> 00:02:16.010
and those who finish
at sometime after

55
00:02:16.010 --> 00:02:17.990
that we'll call them
late finishers.

56
00:02:17.990 --> 00:02:19.505
So I'll create some new variable,

57
00:02:19.505 --> 00:02:22.640
early finishers equals DataFrame.

58
00:02:22.640 --> 00:02:24.875
I'm going to do a pd.to_datetime.

59
00:02:24.875 --> 00:02:27.415
So I'm going to convert
that column to date_time.

60
00:02:27.415 --> 00:02:29.030
We could have done
this when we read in

61
00:02:29.030 --> 00:02:30.500
the CSV file as well.

62
00:02:30.500 --> 00:02:33.050
I want to take that
assignment one submission

63
00:02:33.050 --> 00:02:35.660
time and just if
it's less than 2016.

64
00:02:35.660 --> 00:02:38.795
So then let's take a look
at our early finishers.

65
00:02:38.795 --> 00:02:41.990
So you've got lots of
skills now with pandas.

66
00:02:41.990 --> 00:02:43.550
How would you go about getting

67
00:02:43.550 --> 00:02:45.515
the late finishers DataFrame?

68
00:02:45.515 --> 00:02:48.810
Why don't you pause
this video and give it a try?

69
00:02:49.310 --> 00:02:51.705
All right, here's my solution.

70
00:02:51.705 --> 00:02:53.930
First, the DataFrame df and

71
00:02:53.930 --> 00:02:56.570
the early finishers
share index values.

72
00:02:56.570 --> 00:02:58.370
So I really just
want everything in

73
00:02:58.370 --> 00:03:01.535
the df which is not
in early finishers.

74
00:03:01.535 --> 00:03:03.920
So I'll create late finishers
and I'll make that

75
00:03:03.920 --> 00:03:06.215
equal to our original DataFrame,

76
00:03:06.215 --> 00:03:08.525
and then I'll take the inverse of

77
00:03:08.525 --> 00:03:13.400
df.index.isin
early_finishers.index.

78
00:03:13.400 --> 00:03:18.005
So here the tilde is
a bit wise compliment.

79
00:03:18.005 --> 00:03:19.760
So we're just taking all of

80
00:03:19.760 --> 00:03:22.100
our true values and
negating them to false and

81
00:03:22.100 --> 00:03:23.600
taking all our false values

82
00:03:23.600 --> 00:03:26.585
and negating them back
into true values.

83
00:03:26.585 --> 00:03:28.800
Let's take a look at the head.

84
00:03:28.930 --> 00:03:31.910
So there are a lot of
other ways to do this.

85
00:03:31.910 --> 00:03:33.560
For instance, you
could just copy and

86
00:03:33.560 --> 00:03:35.195
paste the first
projection and change

87
00:03:35.195 --> 00:03:37.820
the sign from less than two
greater than or equal to.

88
00:03:37.820 --> 00:03:38.660
This is okay.

89
00:03:38.660 --> 00:03:40.730
But if you decide you want
to change the date down

90
00:03:40.730 --> 00:03:43.505
the road you have to remember
to change it in two places.

91
00:03:43.505 --> 00:03:45.200
You could also do a join of

92
00:03:45.200 --> 00:03:47.255
the DataFrame df with
early finishers.

93
00:03:47.255 --> 00:03:49.175
If you do a left join you only

94
00:03:49.175 --> 00:03:51.020
keep the items in
the left DataFrame.

95
00:03:51.020 --> 00:03:53.180
So this would have
been a good answer.

96
00:03:53.180 --> 00:03:55.340
You also could have
written a function that

97
00:03:55.340 --> 00:03:57.440
determines if someone
is early or late and

98
00:03:57.440 --> 00:03:59.404
then call.apply on the DataFrame

99
00:03:59.404 --> 00:04:01.610
and added a new column
to the DataFrame.

100
00:04:01.610 --> 00:04:03.935
This is a pretty
reasonable answer as well.

101
00:04:03.935 --> 00:04:05.990
So there's a number of
different things you could have

102
00:04:05.990 --> 00:04:09.025
done to create this DataFrame.

103
00:04:09.025 --> 00:04:12.400
As you've seen, the pandas
DataFrame object has

104
00:04:12.400 --> 00:04:15.475
a variety of statistical
functions associated with it.

105
00:04:15.475 --> 00:04:18.370
If we call the mean function
directly on the DataFrame,

106
00:04:18.370 --> 00:04:19.840
we see that each of the means for

107
00:04:19.840 --> 00:04:21.535
the assignments are calculated.

108
00:04:21.535 --> 00:04:24.235
Let's compare the means
for our two populations.

109
00:04:24.235 --> 00:04:26.515
So I'm just going to
print the early finishers

110
00:04:26.515 --> 00:04:28.705
assignment1_grade.mean.

111
00:04:28.705 --> 00:04:30.760
I'm going to do
the same thing with

112
00:04:30.760 --> 00:04:33.230
the late finishers and
let's take a look.

113
00:04:33.230 --> 00:04:37.405
So these look pretty similar
but are they the same?

114
00:04:37.405 --> 00:04:39.220
What do we mean by similar?

115
00:04:39.220 --> 00:04:41.605
This is where the
student's t-test comes in.

116
00:04:41.605 --> 00:04:43.030
It allows us to form

117
00:04:43.030 --> 00:04:45.610
the alternative hypotheses,
these are different,

118
00:04:45.610 --> 00:04:47.575
as well as the null hypothesis,

119
00:04:47.575 --> 00:04:51.315
these are the same, and then
test that null hypothesis.

120
00:04:51.315 --> 00:04:53.540
When doing hypothesis testing,

121
00:04:53.540 --> 00:04:55.670
we have to choose
a significance level as

122
00:04:55.670 --> 00:04:57.200
a threshold for how much of

123
00:04:57.200 --> 00:04:59.390
a chance we're willing to accept.

124
00:04:59.390 --> 00:05:02.905
This significance level is
typically called Alpha.

125
00:05:02.905 --> 00:05:05.120
For this example, yet let's use

126
00:05:05.120 --> 00:05:07.760
a threshold of 0.05
for our Alpha,

127
00:05:07.760 --> 00:05:09.170
which is five percent.

128
00:05:09.170 --> 00:05:10.880
Now this is commonly used number

129
00:05:10.880 --> 00:05:13.030
but it's really quite arbitrary.

130
00:05:13.030 --> 00:05:15.560
The SciPy library
contains a number of

131
00:05:15.560 --> 00:05:17.840
different statistical
tests and forms

132
00:05:17.840 --> 00:05:20.555
a basis for hypothesis
testing in Python.

133
00:05:20.555 --> 00:05:23.300
We're going to use the
ttest_ind() function,

134
00:05:23.300 --> 00:05:25.235
which does an independent t-test,

135
00:05:25.235 --> 00:05:26.780
meaning that the populations in

136
00:05:26.780 --> 00:05:29.555
the two groups are not
related to one another.

137
00:05:29.555 --> 00:05:31.485
The result of t-ttest_ind()

138
00:05:31.485 --> 00:05:34.310
are this t statistic
and the p-value.

139
00:05:34.310 --> 00:05:35.875
It's this latter value

140
00:05:35.875 --> 00:05:38.920
the probability which is
most important to us as it

141
00:05:38.920 --> 00:05:41.290
indicates the chance
between zero and

142
00:05:41.290 --> 00:05:44.080
one of our null
hypothesis being true.

143
00:05:44.080 --> 00:05:46.680
So let's bring in
ttest_ind() function.

144
00:05:46.680 --> 00:05:49.815
So from SciPy stats
import ttest_ind().

145
00:05:49.815 --> 00:05:52.900
Let's run this function
with our two populations.

146
00:05:52.900 --> 00:05:54.670
We're going to look at
the assignment one grades.

147
00:05:54.670 --> 00:05:56.350
So ttest_ind(), we'll

148
00:05:56.350 --> 00:05:58.510
take early finishers and
we just want to project

149
00:05:58.510 --> 00:06:00.820
the assignment1_grade
and late finishers

150
00:06:00.820 --> 00:06:03.565
and we'll project
the assignment1_grade.

151
00:06:03.565 --> 00:06:08.335
So here we see that
the probability is 0.18.

152
00:06:08.335 --> 00:06:12.790
This is above our
Alpha value of 0.05.

153
00:06:12.790 --> 00:06:16.310
This means that we cannot
reject the null hypothesis.

154
00:06:16.310 --> 00:06:18.290
The null hypothesis was

155
00:06:18.290 --> 00:06:20.390
that the two populations
are the same.

156
00:06:20.390 --> 00:06:23.705
We don't have enough certainty
in our evidence because

157
00:06:23.705 --> 00:06:25.400
the probability is greater than

158
00:06:25.400 --> 00:06:28.475
Alpha to come to a conclusion
to the contrary.

159
00:06:28.475 --> 00:06:30.485
This doesn't mean
that we've proven

160
00:06:30.485 --> 00:06:32.840
that the populations
are the same.

161
00:06:32.840 --> 00:06:36.780
So why don't we check
the other assignment grades.

162
00:06:36.780 --> 00:06:38.810
So I'm just going to
copy and paste here.

163
00:06:38.810 --> 00:06:41.135
So I'm going to put it in
the assignment2_grade,

164
00:06:41.135 --> 00:06:44.540
assignment3_grade
assignment4 _ grade,

165
00:06:44.540 --> 00:06:48.175
five and I think there
are six assignments here.

166
00:06:48.175 --> 00:06:52.220
Okay, so it looks like in
this data we do not have

167
00:06:52.220 --> 00:06:53.750
enough evidence to suggest

168
00:06:53.750 --> 00:06:56.510
the populations differ
with respect to grade.

169
00:06:56.510 --> 00:06:58.280
Let's look at those p-values

170
00:06:58.280 --> 00:06:59.855
for a moment though
because they're saying

171
00:06:59.855 --> 00:07:03.155
things that can inform
experimental design down the road.

172
00:07:03.155 --> 00:07:05.000
For instance, one
of the assignments,

173
00:07:05.000 --> 00:07:08.630
assignment3 has a
p-value around 0.1.

174
00:07:08.630 --> 00:07:10.910
This means that if
we accept a level of

175
00:07:10.910 --> 00:07:13.355
chance similarity of 11 percent,

176
00:07:13.355 --> 00:07:16.600
this would have been considered
statistically significant.

177
00:07:16.600 --> 00:07:19.715
As a researcher, this would
suggest to me that there's

178
00:07:19.715 --> 00:07:21.170
a number of things here worth

179
00:07:21.170 --> 00:07:23.060
considering following up on.

180
00:07:23.060 --> 00:07:25.895
For instance, if we had
a small number of participants,

181
00:07:25.895 --> 00:07:27.125
and we don't here,

182
00:07:27.125 --> 00:07:28.490
or if there was something unique

183
00:07:28.490 --> 00:07:29.930
about this assignment as it

184
00:07:29.930 --> 00:07:33.555
relates to our experiment
whatever the experiment was,

185
00:07:33.555 --> 00:07:35.960
then there may be
followup experiments

186
00:07:35.960 --> 00:07:39.155
that we could run to better
understand the phenomenon.

187
00:07:39.155 --> 00:07:41.540
Now, p-values have come under

188
00:07:41.540 --> 00:07:43.730
fire recently for
being insufficient for

189
00:07:43.730 --> 00:07:45.890
telling us enough
about the interactions

190
00:07:45.890 --> 00:07:48.320
which are happening and
two other techniques

191
00:07:48.320 --> 00:07:51.605
confidence intervals
and Bayesian analyses

192
00:07:51.605 --> 00:07:53.540
are being used more regularly.

193
00:07:53.540 --> 00:07:55.640
One issue with p-values is that

194
00:07:55.640 --> 00:07:57.620
as you run more
tests you're likely

195
00:07:57.620 --> 00:07:59.000
to get a value which is

196
00:07:59.000 --> 00:08:02.000
statistically significant
just by chance.

197
00:08:02.000 --> 00:08:04.340
So let's see a little
simulation of this.

198
00:08:04.340 --> 00:08:07.805
First, let's create
a DataFrame of 100 columns

199
00:08:07.805 --> 00:08:09.775
each with a 100 numbers.

200
00:08:09.775 --> 00:08:14.880
So here I'm going to
do dF1 pd.DataFrame,

201
00:08:14.880 --> 00:08:18.020
and then we'll put in
a nice list comprehension.

202
00:08:18.020 --> 00:08:22.415
Np.random.normal random
100 for x in range 100.

203
00:08:22.415 --> 00:08:24.385
Let's look at the head of that.

204
00:08:24.385 --> 00:08:27.140
Now, pause this and reflect.

205
00:08:27.140 --> 00:08:30.050
Do you understand
the list comprehension

206
00:08:30.050 --> 00:08:32.275
and how I created this DataFrame?

207
00:08:32.275 --> 00:08:35.420
You don't have to use a list
comprehension to do this,

208
00:08:35.420 --> 00:08:37.070
but you should be
able to read this

209
00:08:37.070 --> 00:08:38.900
and figure it out because this is

210
00:08:38.900 --> 00:08:40.880
a commonly used
approach that you'll

211
00:08:40.880 --> 00:08:43.655
find on web forms
and then help forms.

212
00:08:43.655 --> 00:08:46.220
Okay. Let's create
a second DataFrame.

213
00:08:46.220 --> 00:08:49.220
So here we'll just do
the same thing dF2 equals

214
00:08:49.220 --> 00:08:52.370
pd.DataFrame and I could
have a list comprehension.

215
00:08:52.370 --> 00:08:54.695
What I'm saying here is I want to

216
00:08:54.695 --> 00:08:58.925
call the statement
np.random.random100.

217
00:08:58.925 --> 00:09:01.580
So this will generate
100 random values into it in

218
00:09:01.580 --> 00:09:04.700
a list for x in range 100.

219
00:09:04.700 --> 00:09:08.345
So I want to iterate over
another list of 100 values.

220
00:09:08.345 --> 00:09:10.520
I'm actually not using X here.

221
00:09:10.520 --> 00:09:12.650
It's just being thrown away
because the data that I'm

222
00:09:12.650 --> 00:09:15.890
using is the np.random.random.

223
00:09:15.890 --> 00:09:19.655
So are these two
DataFrames the same?

224
00:09:19.655 --> 00:09:22.790
Maybe a better question is
for a given row inside of

225
00:09:22.790 --> 00:09:27.065
dF1 is it the same as
that same row inside of df2.

226
00:09:27.065 --> 00:09:30.130
So let's take a look. Let's
say our critical value

227
00:09:30.130 --> 00:09:33.210
here is 0.1 or an Alpha
of 10 percent.

228
00:09:33.210 --> 00:09:35.440
We're going to compare
each column in

229
00:09:35.440 --> 00:09:38.020
dF1 to the same
numbered column in

230
00:09:38.020 --> 00:09:40.510
df2 and we'll report when

231
00:09:40.510 --> 00:09:43.225
the p-value isn't
less than 10 percent,

232
00:09:43.225 --> 00:09:45.250
which means that we have
sufficient evidence

233
00:09:45.250 --> 00:09:47.065
to say that the columns
are different.

234
00:09:47.065 --> 00:09:50.125
So let's write this as a
function called test_columns.

235
00:09:50.125 --> 00:09:51.790
So def tests_columns.

236
00:09:51.790 --> 00:09:53.440
We'll pass a parameter Alpha

237
00:09:53.440 --> 00:09:55.240
by default we'll set it to 0.1.

238
00:09:55.240 --> 00:09:57.250
We can change that later though,

239
00:09:57.250 --> 00:09:58.855
so it's nice to have a parameter.

240
00:09:58.855 --> 00:10:01.710
I want to keep track of how
many columns actually differ,

241
00:10:01.710 --> 00:10:04.905
so make some new variable
num_diff and make it zero.

242
00:10:04.905 --> 00:10:07.105
Now, we're just going to
iterate over the columns.

243
00:10:07.105 --> 00:10:09.670
So for call in dF1.columns,

244
00:10:09.670 --> 00:10:12.550
we're just iterating over
all the list of columns in

245
00:10:12.550 --> 00:10:14.380
dF1 we're just going to run

246
00:10:14.380 --> 00:10:17.140
our t-test in between
the two DataFrames.

247
00:10:17.140 --> 00:10:19.570
So we're going to
get a test stat and

248
00:10:19.570 --> 00:10:22.260
p-value is equal to ttests_ind().

249
00:10:22.260 --> 00:10:25.650
We'll do dF1 sub call
and df2 sub call.

250
00:10:25.650 --> 00:10:28.080
Remember t-tests end
returns two values.

251
00:10:28.080 --> 00:10:30.700
We can use tuple
unpacking here to get

252
00:10:30.700 --> 00:10:34.165
the test stat in its variable
and the probability in it.

253
00:10:34.165 --> 00:10:37.050
We're going to check the
p-value versus the Alphas,

254
00:10:37.050 --> 00:10:40.719
so if the p-value is less
than or equal to Alpha

255
00:10:40.719 --> 00:10:42.520
and now we're just going
to print out if they're

256
00:10:42.520 --> 00:10:44.560
different and increment
the number of difference.

257
00:10:44.560 --> 00:10:47.424
So column is statistically

258
00:10:47.424 --> 00:10:49.855
significantly different
at some Alpha level,

259
00:10:49.855 --> 00:10:52.795
some pval level and will
pass our parameters.

260
00:10:52.795 --> 00:10:56.050
Of course, let's increment
our number of differences.

261
00:10:56.050 --> 00:10:58.395
Let's print out
some summary stats

262
00:10:58.395 --> 00:11:00.620
after were done testing
all the columns.

263
00:11:00.620 --> 00:11:02.360
So the total number
of different ones

264
00:11:02.360 --> 00:11:04.940
we'll place that in
and we'll turn it

265
00:11:04.940 --> 00:11:06.740
into a percentage and we'll

266
00:11:06.740 --> 00:11:10.180
just do the math with
the length of columns.

267
00:11:10.180 --> 00:11:12.470
Now let's actually run this code.

268
00:11:12.470 --> 00:11:14.730
So ttests_columns.

269
00:11:14.770 --> 00:11:17.450
Interesting. So we
see that there's

270
00:11:17.450 --> 00:11:19.760
a bunch of columns that
are actually different.

271
00:11:19.760 --> 00:11:21.455
In fact, that number looks

272
00:11:21.455 --> 00:11:24.155
a lot like the Alpha
value we chose.

273
00:11:24.155 --> 00:11:26.090
So what's going on shouldn't

274
00:11:26.090 --> 00:11:27.980
all of the columns be the same?

275
00:11:27.980 --> 00:11:31.790
Remember, that
all t-test does is check if

276
00:11:31.790 --> 00:11:34.040
two sets are similar given

277
00:11:34.040 --> 00:11:37.150
some level of confidence
in our case 10 percent.

278
00:11:37.150 --> 00:11:39.725
The more random
comparisons you do,

279
00:11:39.725 --> 00:11:42.575
the more will just happen
to be the same by chance.

280
00:11:42.575 --> 00:11:45.710
In this example, we
checked 100 columns.

281
00:11:45.710 --> 00:11:47.975
So we would expect there
to be roughly 10 of

282
00:11:47.975 --> 00:11:51.455
them to be the same
if our Alpha was 0.1.

283
00:11:51.455 --> 00:11:54.485
So we can test some other
Alpha values as well.

284
00:11:54.485 --> 00:11:56.285
So let's do tests columns with

285
00:11:56.285 --> 00:12:00.300
0.05 and you could try
other values as well.

286
00:12:00.640 --> 00:12:03.380
So keep this in mind
when you're doing

287
00:12:03.380 --> 00:12:06.755
statistical tests like
the t-test which has a p-value.

288
00:12:06.755 --> 00:12:10.085
Understand that this p-value
isn't magic and it has

289
00:12:10.085 --> 00:12:12.080
a threshold for
you when reporting

290
00:12:12.080 --> 00:12:14.635
results and trying to
answer your hypothesis.

291
00:12:14.635 --> 00:12:16.665
What's a reasonable threshold?

292
00:12:16.665 --> 00:12:19.520
That depends on your question
and you need to engage

293
00:12:19.520 --> 00:12:20.930
domain experts to better

294
00:12:20.930 --> 00:12:24.245
understand what they would
consider significant.

295
00:12:24.245 --> 00:12:26.510
So just for fun, let's recreate

296
00:12:26.510 --> 00:12:29.810
that second DataFrame using
a non-normal distribution.

297
00:12:29.810 --> 00:12:32.000
I'm going to arbitrarily
choose chi-squared.

298
00:12:32.000 --> 00:12:33.770
You can try some other ones
if you'd like.

299
00:12:33.770 --> 00:12:36.530
So we'll just df2
equals pd.DataFrame

300
00:12:36.530 --> 00:12:39.545
and now we'll just do
np.random.chi-squared.

301
00:12:39.545 --> 00:12:41.510
Now chi-squared is
a distribution actually

302
00:12:41.510 --> 00:12:44.090
takes parameter
the degrees of freedom.

303
00:12:44.090 --> 00:12:45.230
I'll just set it to one here.

304
00:12:45.230 --> 00:12:48.560
You can read about that or
maybe you already know about

305
00:12:48.560 --> 00:12:51.380
the chi-squared distribution
and I want 100 values

306
00:12:51.380 --> 00:12:54.380
and we're going to iterate
over 100 columns as well.

307
00:12:54.380 --> 00:12:58.735
Let's just test that.
So now we see that

308
00:12:58.735 --> 00:13:01.210
all or most columns tests to be

309
00:13:01.210 --> 00:13:04.910
statistically significant
at the 10 percent level.

310
00:13:05.390 --> 00:13:08.380
In this lecture, we've
discussed just some of

311
00:13:08.380 --> 00:13:10.990
the basics of hypothesis
testing in Python.

312
00:13:10.990 --> 00:13:13.020
I introduced you to
the SciPy library.

313
00:13:13.020 --> 00:13:14.800
Which you can use for
the student's t-tests.

314
00:13:14.800 --> 00:13:17.500
We've discussed some of
the practical issues

315
00:13:17.500 --> 00:13:20.470
which arise from looking at
statistical significance.

316
00:13:20.470 --> 00:13:23.995
Now, there's much more to learn
about hypothesis testing.

317
00:13:23.995 --> 00:13:26.410
For instance, there are
different tests to be used

318
00:13:26.410 --> 00:13:29.185
depending on the shape of
your data and different ways to

319
00:13:29.185 --> 00:13:30.700
report on the results instead of

320
00:13:30.700 --> 00:13:32.320
just p values such as

321
00:13:32.320 --> 00:13:35.005
confidence intervals
or Bayesian analysis.

322
00:13:35.005 --> 00:13:37.060
But this should give
you a basic idea

323
00:13:37.060 --> 00:13:38.410
of where to start when comparing

324
00:13:38.410 --> 00:13:40.300
two populations for differences

325
00:13:40.300 --> 00:13:43.880
which is a common task
for data scientists.