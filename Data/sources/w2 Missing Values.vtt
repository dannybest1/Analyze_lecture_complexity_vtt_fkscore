WEBVTT

1
00:00:05.180 --> 00:00:08.790
We've seen a preview of how
Pandas handles missing values

2
00:00:08.790 --> 00:00:11.970
using the None Type and the
NumPy and NaN keywords.

3
00:00:11.970 --> 00:00:13.680
Missing values are pretty common

4
00:00:13.680 --> 00:00:15.135
in data cleaning activities,

5
00:00:15.135 --> 00:00:16.380
and missing values can be there

6
00:00:16.380 --> 00:00:17.640
for any number of reasons,

7
00:00:17.640 --> 00:00:20.445
and I just want to touch
on a few of those here.

8
00:00:20.445 --> 00:00:22.830
For instance, if you're
running a survey and

9
00:00:22.830 --> 00:00:24.839
a respondent didn't
answer a question,

10
00:00:24.839 --> 00:00:27.345
the missing value is
actually an omission.

11
00:00:27.345 --> 00:00:29.070
This kind of
missing data is called

12
00:00:29.070 --> 00:00:30.705
missing at random if there are

13
00:00:30.705 --> 00:00:32.430
other variables that
might be used to

14
00:00:32.430 --> 00:00:34.620
predict the variable
which is missing.

15
00:00:34.620 --> 00:00:38.010
In my work when I deliver
surveys I often find that

16
00:00:38.010 --> 00:00:39.960
missing data say interest

17
00:00:39.960 --> 00:00:42.090
in being involved in
a follow-up study,

18
00:00:42.090 --> 00:00:44.225
often has some correlation with

19
00:00:44.225 --> 00:00:46.730
other data like
gender or ethnicity.

20
00:00:46.730 --> 00:00:49.490
If there's no relationship
to other variables,

21
00:00:49.490 --> 00:00:53.480
then we call this data
missing completely at random.

22
00:00:53.480 --> 00:00:55.520
So these are just two examples of

23
00:00:55.520 --> 00:00:57.365
missing data and
there's many more.

24
00:00:57.365 --> 00:00:58.880
For instance, data might be

25
00:00:58.880 --> 00:01:00.430
missing because it
wasn't collected.

26
00:01:00.430 --> 00:01:02.420
Either because the process
responsible for

27
00:01:02.420 --> 00:01:05.150
collecting the data
such as the researcher,

28
00:01:05.150 --> 00:01:06.830
or because it wouldn't make sense

29
00:01:06.830 --> 00:01:08.630
if it were to be collected.

30
00:01:08.630 --> 00:01:10.730
This last example is extremely

31
00:01:10.730 --> 00:01:12.740
common when you start
joining DataFrames

32
00:01:12.740 --> 00:01:15.800
together from multiple sources
such as joining a list of

33
00:01:15.800 --> 00:01:17.540
people at a university with

34
00:01:17.540 --> 00:01:19.745
a list of offices
in the university.

35
00:01:19.745 --> 00:01:21.575
Students don't generally have

36
00:01:21.575 --> 00:01:24.310
offices but they're still
people at the university.

37
00:01:24.310 --> 00:01:26.450
So let's take a look
at some ways of

38
00:01:26.450 --> 00:01:29.035
handling missing data in Pandas.

39
00:01:29.035 --> 00:01:33.275
So let's import Pandas, 
import pandas as pd.

40
00:01:33.275 --> 00:01:36.230
So Pandas is pretty good at
detecting missing values

41
00:01:36.230 --> 00:01:39.560
directly from underlying
data formats like CSV files.

42
00:01:39.560 --> 00:01:41.630
Although most missing
values are often

43
00:01:41.630 --> 00:01:43.505
formatted as NaN, NULL,

44
00:01:43.505 --> 00:01:44.990
None, or N/A,

45
00:01:44.990 --> 00:01:48.035
sometimes missing values
are not labeled so clearly.

46
00:01:48.035 --> 00:01:49.415
For example, I've worked with

47
00:01:49.415 --> 00:01:52.010
social scientists who
regularly use the value of

48
00:01:52.010 --> 00:01:54.530
99 in binary categories

49
00:01:54.530 --> 00:01:56.690
to indicate that it's
a missing value.

50
00:01:56.690 --> 00:01:59.880
The Pandas read csv function
has a parameter called

51
00:01:59.880 --> 00:02:01.190
na_values that allows us to

52
00:02:01.190 --> 00:02:03.620
specify the format
of missing values.

53
00:02:03.620 --> 00:02:06.800
It allows scalar string lists
or dictionaries to be used.

54
00:02:06.800 --> 00:02:10.870
So let's load a piece of data
from a file called log.csv.

55
00:02:10.870 --> 00:02:15.220
So df equals pd.read csv
and we'll say datasets,

56
00:02:15.220 --> 00:02:17.905
actually will use
class grades.csv.

57
00:02:17.905 --> 00:02:20.560
If we do df.head
and we pass in 10,

58
00:02:20.560 --> 00:02:22.925
we can see the top 10 rows.

59
00:02:22.925 --> 00:02:24.885
So we can actually use the

60
00:02:24.885 --> 00:02:27.760
functions .isnull to
create a Boolean mask

61
00:02:27.760 --> 00:02:29.035
of the whole DataFrame.

62
00:02:29.035 --> 00:02:32.725
This effectively broadcasts
the isnull function

63
00:02:32.725 --> 00:02:34.285
to every cell of data.

64
00:02:34.285 --> 00:02:37.180
So we'll say mask
equals df.isnull,

65
00:02:37.180 --> 00:02:39.235
remember this is now just that

66
00:02:39.235 --> 00:02:40.450
isnull function has been

67
00:02:40.450 --> 00:02:42.760
broadcast across
that whole dataframe,

68
00:02:42.760 --> 00:02:45.370
and what we've gotten mask
is something that size and

69
00:02:45.370 --> 00:02:48.250
shape of the DataFrame
but it's a Boolean mask,

70
00:02:48.250 --> 00:02:50.510
and we can take a look
at the head of that.

71
00:02:50.510 --> 00:02:53.300
So this can be useful
for processing

72
00:02:53.300 --> 00:02:55.925
rows based on certain
columns of data.

73
00:02:55.925 --> 00:02:58.550
Another useful operation
is to be able to drop

74
00:02:58.550 --> 00:03:00.980
all of those rows which
have any missing data,

75
00:03:00.980 --> 00:03:03.085
which can be done with
the dropna function.

76
00:03:03.085 --> 00:03:06.895
So we can say df.dropna.head.

77
00:03:06.895 --> 00:03:09.665
So note how the rows
index with 2,

78
00:03:09.665 --> 00:03:12.200
3, 7, 1,1 are all gone now.

79
00:03:12.200 --> 00:03:13.730
One of the handy functions

80
00:03:13.730 --> 00:03:15.170
that Pandas has for working with

81
00:03:15.170 --> 00:03:18.440
missing values is the filling
function called fillna.

82
00:03:18.440 --> 00:03:21.365
This function takes
a number of parameters.

83
00:03:21.365 --> 00:03:24.080
You can pass in a single value
which is called a scalar

84
00:03:24.080 --> 00:03:27.155
value to change all of
the missing data to one value.

85
00:03:27.155 --> 00:03:29.120
This isn't really
applicable in this case,

86
00:03:29.120 --> 00:03:31.155
but it's a pretty
common use case.

87
00:03:31.155 --> 00:03:32.795
So if we wanted to fill in

88
00:03:32.795 --> 00:03:35.555
all missing values with
zero we could use fillna.

89
00:03:35.555 --> 00:03:37.910
So we just say df.fillna,

90
00:03:37.910 --> 00:03:39.410
we pass in a zero,

91
00:03:39.410 --> 00:03:41.600
here I'll just change
the DataFrame in place.

92
00:03:41.600 --> 00:03:43.880
Remember most
DataFrame operations

93
00:03:43.880 --> 00:03:45.800
returned copies of DataFrames.

94
00:03:45.800 --> 00:03:47.855
So if you want to
do it in place you

95
00:03:47.855 --> 00:03:49.880
often have to use the
in-place parameter,

96
00:03:49.880 --> 00:03:52.550
and then let's take a look
at the head of that.

97
00:03:52.550 --> 00:03:56.330
So note that the in-place
attribute causes Pandas to fill

98
00:03:56.330 --> 00:03:57.980
those values in line and it does

99
00:03:57.980 --> 00:04:00.110
not return a copy
of the DataFrame,

100
00:04:00.110 --> 00:04:03.610
but instead modifies
the DataFrame that you have.

101
00:04:03.610 --> 00:04:06.620
We can also use
the na filter option

102
00:04:06.620 --> 00:04:08.555
to turn off whitespace filtering.

103
00:04:08.555 --> 00:04:11.480
If whitespace is an actual
value of interest,

104
00:04:11.480 --> 00:04:13.790
but in practice this
is pretty rare.

105
00:04:13.790 --> 00:04:16.970
In data without any
na's passing na filter

106
00:04:16.970 --> 00:04:18.350
equals false can improve

107
00:04:18.350 --> 00:04:21.200
the performance of
reading a large file.

108
00:04:21.200 --> 00:04:23.480
In addition to rules controlling

109
00:04:23.480 --> 00:04:25.220
how missing values
might be loaded,

110
00:04:25.220 --> 00:04:26.870
it's sometimes useful to consider

111
00:04:26.870 --> 00:04:29.660
missing values as actually
having information.

112
00:04:29.660 --> 00:04:31.805
I'll give an example
from my own research.

113
00:04:31.805 --> 00:04:34.910
I often deal with logs from
online learning systems.

114
00:04:34.910 --> 00:04:38.015
I've looked at video use in
lecture capture systems.

115
00:04:38.015 --> 00:04:40.430
In these systems, it's
common for the player

116
00:04:40.430 --> 00:04:42.770
to have a heartbeat
functionality,

117
00:04:42.770 --> 00:04:44.360
where playbacks
statistics are sent

118
00:04:44.360 --> 00:04:46.090
to the server every so often,

119
00:04:46.090 --> 00:04:47.825
maybe every 30 seconds.

120
00:04:47.825 --> 00:04:50.270
These heartbeats can
get big as they carry

121
00:04:50.270 --> 00:04:52.530
the whole state of
the playback system.

122
00:04:52.530 --> 00:04:54.230
Such as where the video
play head is at,

123
00:04:54.230 --> 00:04:55.790
to where the video sizes at.

124
00:04:55.790 --> 00:04:57.560
Where the video is being
rendered to the screen.

125
00:04:57.560 --> 00:04:59.660
How loud the volume is and so on.

126
00:04:59.660 --> 00:05:01.280
So if we load the data file

127
00:05:01.280 --> 00:05:03.785
log.csv we can see
an example of this.

128
00:05:03.785 --> 00:05:06.685
So df equals pd.read_csv,

129
00:05:06.685 --> 00:05:10.910
will bring in log.csv and
we'll take a look at the head.

130
00:05:12.970 --> 00:05:15.710
So in this data
the first column is

131
00:05:15.710 --> 00:05:18.470
a timestamp in
the Unix epoch format.

132
00:05:18.470 --> 00:05:20.660
The next column is
the username followed by

133
00:05:20.660 --> 00:05:22.175
a webpage they're visiting

134
00:05:22.175 --> 00:05:23.645
and the video that
they're playing.

135
00:05:23.645 --> 00:05:26.580
Each row of the DataFrame
has a playback position.

136
00:05:26.580 --> 00:05:28.580
We can see that as
the playback position

137
00:05:28.580 --> 00:05:29.750
increases by one,

138
00:05:29.750 --> 00:05:33.595
the timestamp increases
by about 30 seconds,

139
00:05:33.595 --> 00:05:35.525
except for user Bob .

140
00:05:35.525 --> 00:05:38.000
It turns out that Bob
has paused his playback,

141
00:05:38.000 --> 00:05:39.380
so as time increases

142
00:05:39.380 --> 00:05:41.660
the playback position
doesn't change.

143
00:05:41.660 --> 00:05:43.940
Note too how difficult
it is for us to

144
00:05:43.940 --> 00:05:46.300
try and derive this
knowledge from the data,

145
00:05:46.300 --> 00:05:49.835
because it's not sorted by
timestamp as one might expect.

146
00:05:49.835 --> 00:05:51.725
This is actually not uncommon on

147
00:05:51.725 --> 00:05:54.410
systems which have
a high degree of parallelism.

148
00:05:54.410 --> 00:05:56.300
There are a lot of
missing values in

149
00:05:56.300 --> 00:05:58.370
the paused and volume columns.

150
00:05:58.370 --> 00:05:59.990
It's not efficient to send

151
00:05:59.990 --> 00:06:02.360
this information across
the network if it hasn't change,

152
00:06:02.360 --> 00:06:04.775
so implementations rarely do.

153
00:06:04.775 --> 00:06:07.670
So this particular
system just inserts

154
00:06:07.670 --> 00:06:11.285
null values into the database
if there's no changes.

155
00:06:11.285 --> 00:06:14.270
So next up is
the method parameter.

156
00:06:14.270 --> 00:06:17.980
The two common fill values
are ffill and bfill.

157
00:06:17.980 --> 00:06:19.595
Ffill is for forward

158
00:06:19.595 --> 00:06:21.590
filling and it updates
an na value for

159
00:06:21.590 --> 00:06:25.345
a particular cell with the
value from the previous row.

160
00:06:25.345 --> 00:06:26.990
bfills for backward filling

161
00:06:26.990 --> 00:06:28.655
which is the opposite
of ffill.

162
00:06:28.655 --> 00:06:31.970
It fills the missing values
with the next valid value.

163
00:06:31.970 --> 00:06:34.430
It's important to note
that your data needs to be

164
00:06:34.430 --> 00:06:35.810
sorted in order for this to

165
00:06:35.810 --> 00:06:37.625
have the effect you might want.

166
00:06:37.625 --> 00:06:38.750
Data which comes from

167
00:06:38.750 --> 00:06:40.625
traditional database
management systems

168
00:06:40.625 --> 00:06:43.320
usually has no order guarantee
just like this data,

169
00:06:43.320 --> 00:06:44.930
so you have to be careful.

170
00:06:44.930 --> 00:06:48.075
So in Pandas we can sort
by index or by value.

171
00:06:48.075 --> 00:06:49.820
Here will just promote
the timestamp to

172
00:06:49.820 --> 00:06:51.710
an index and then
sort on the index.

173
00:06:51.710 --> 00:06:55.190
So I'm going to say df
equals df.set index,

174
00:06:55.190 --> 00:06:56.554
and I want the timestamp,

175
00:06:56.554 --> 00:07:01.780
and then df.sort index and
let's take the head of that.

176
00:07:01.780 --> 00:07:05.290
So now we have sorted
timestamp data.

177
00:07:05.290 --> 00:07:08.180
If we look closely at
the output though we'll

178
00:07:08.180 --> 00:07:10.880
notice that the index
isn't really unique.

179
00:07:10.880 --> 00:07:12.230
Two users seemed to be able to

180
00:07:12.230 --> 00:07:13.790
use the system at the same time,

181
00:07:13.790 --> 00:07:16.355
and again this is
actually a common case.

182
00:07:16.355 --> 00:07:18.200
So let's reset the index and use

183
00:07:18.200 --> 00:07:22.109
some multi-level indexing
on time and user together,

184
00:07:22.109 --> 00:07:23.420
and promote the username to

185
00:07:23.420 --> 00:07:25.865
a second level index to
deal with the issue.

186
00:07:25.865 --> 00:07:28.595
So I'll just reset
the index of the DataFrame,

187
00:07:28.595 --> 00:07:31.940
and then we'll say df.set
index and we pass in

188
00:07:31.940 --> 00:07:33.590
a list and we want time to be

189
00:07:33.590 --> 00:07:35.420
the top level and then user,

190
00:07:35.420 --> 00:07:38.130
and let's take a look
at that DataFrame.

191
00:07:39.980 --> 00:07:42.380
Now that we have the Data indexed

192
00:07:42.380 --> 00:07:43.700
and sorted appropriately

193
00:07:43.700 --> 00:07:46.655
we can fill the missing
datas using ffill.

194
00:07:46.655 --> 00:07:48.440
It's good to remember
when dealing

195
00:07:48.440 --> 00:07:50.240
with missing values
so that you can deal

196
00:07:50.240 --> 00:07:52.130
with individual
columns or sets of

197
00:07:52.130 --> 00:07:54.320
columns by projecting them.

198
00:07:54.320 --> 00:07:55.640
So you don't have to fix all

199
00:07:55.640 --> 00:07:57.700
missing values in one command.

200
00:07:57.700 --> 00:08:00.690
So df equals df.fillna

201
00:08:00.690 --> 00:08:04.590
and we want to set the method
is equal to ffill.

202
00:08:04.840 --> 00:08:08.240
So we can also do
customize fill-in

203
00:08:08.240 --> 00:08:10.910
to replace values with
the replace function.

204
00:08:10.910 --> 00:08:13.415
It allows replacement
from several approaches.

205
00:08:13.415 --> 00:08:16.565
Value-to-value, list,
dictionary, regex.

206
00:08:16.565 --> 00:08:18.515
So let's generate
a simple example.

207
00:08:18.515 --> 00:08:20.795
So I'm going to create
a DataFrame here,

208
00:08:20.795 --> 00:08:25.110
let say we want column A
to have these values 1,

209
00:08:25.110 --> 00:08:26.670
1, 2, 3, 4.

210
00:08:26.670 --> 00:08:30.030
B we'll say 3, 6, 3,

211
00:08:30.030 --> 00:08:32.160
8, 9, and C

212
00:08:32.160 --> 00:08:35.070
we'll just make a bunch
of different characters.

213
00:08:35.240 --> 00:08:38.580
We can replace one's with 100,

214
00:08:38.580 --> 00:08:40.470
so let's try the
value-to-value approach.

215
00:08:40.470 --> 00:08:42.240
So we just say df.replace.

216
00:08:42.240 --> 00:08:43.780
We give it the first value,

217
00:08:43.780 --> 00:08:45.830
thing we want to replace
and what we want to

218
00:08:45.830 --> 00:08:48.595
replace it with, so 100.

219
00:08:48.595 --> 00:08:50.775
So about changing two values,

220
00:08:50.775 --> 00:08:52.265
so let's try the list approach.

221
00:08:52.265 --> 00:08:54.250
For example, we want
to change 1's

222
00:08:54.250 --> 00:08:56.530
to 100 and 3's to 300.

223
00:08:56.530 --> 00:09:00.250
So we could just do df.replace
and pass in two lists;

224
00:09:00.250 --> 00:09:04.030
1 and 3 and then 100 and 300.

225
00:09:04.030 --> 00:09:05.910
So what's really cool about

226
00:09:05.910 --> 00:09:09.170
Pandas replacement is that
it supports regex too.

227
00:09:09.170 --> 00:09:13.310
So let's look at our dataset
again from the logs.csv.

228
00:09:13.310 --> 00:09:15.650
So pd.read csv will bring

229
00:09:15.650 --> 00:09:19.080
in logs.csv and take
a look at this again.

230
00:09:19.880 --> 00:09:22.770
So to replace using regex,

231
00:09:22.770 --> 00:09:24.815
we make the first
parameter to replace

232
00:09:24.815 --> 00:09:27.275
the regex pattern that
we want to replace.

233
00:09:27.275 --> 00:09:29.330
The second parameter the value

234
00:09:29.330 --> 00:09:31.160
that we want to
emit upon a match.

235
00:09:31.160 --> 00:09:32.990
Then we pass in a third parameter

236
00:09:32.990 --> 00:09:35.240
that just says regex equals true.

237
00:09:35.240 --> 00:09:37.040
So take a moment to pause

238
00:09:37.040 --> 00:09:39.260
this video and think
about this problem.

239
00:09:39.260 --> 00:09:40.970
Imagine that we want to detect

240
00:09:40.970 --> 00:09:43.805
all HTML pages in
the video column.

241
00:09:43.805 --> 00:09:47.240
Let's say that this just means
that they ended with .HTML,

242
00:09:47.240 --> 00:09:50.540
and we want to overwrite that
with the keyword webpage.

243
00:09:50.540 --> 00:09:53.400
How could we accomplish that?

244
00:09:53.870 --> 00:09:56.100
So here's my solution.

245
00:09:56.100 --> 00:09:57.830
First, matching any number of

246
00:09:57.830 --> 00:10:00.665
characters then ending in .HTML.

247
00:10:00.665 --> 00:10:02.600
So I'll do df.replace.

248
00:10:02.600 --> 00:10:04.280
I say to_replace,

249
00:10:04.280 --> 00:10:06.980
so I'm going to put
in here that pattern,

250
00:10:06.980 --> 00:10:10.400
any number of characters
so .star, .HTML,

251
00:10:10.400 --> 00:10:11.615
and then I don't want to get out

252
00:10:11.615 --> 00:10:13.490
things that have dot
HTML in the middle,

253
00:10:13.490 --> 00:10:15.425
so I'll anchor this to the end.

254
00:10:15.425 --> 00:10:17.660
The new value I want
is called webpage,

255
00:10:17.660 --> 00:10:20.840
so value is webpage and
just regex equals true.

256
00:10:20.840 --> 00:10:24.930
We see that that does
the trick there.

257
00:10:25.460 --> 00:10:28.305
So one last note
on missing values.

258
00:10:28.305 --> 00:10:31.130
When you use statistical
functions on DataFrames,

259
00:10:31.130 --> 00:10:34.190
these functions typically
ignore missing values.

260
00:10:34.190 --> 00:10:35.870
For instance, if you
try and calculate

261
00:10:35.870 --> 00:10:37.070
the mean value of a DataFrame

262
00:10:37.070 --> 00:10:39.050
the underlying NumPy functions

263
00:10:39.050 --> 00:10:41.390
may ignore those missing values.

264
00:10:41.390 --> 00:10:43.070
This is usually what you want,

265
00:10:43.070 --> 00:10:44.435
but you should be aware of

266
00:10:44.435 --> 00:10:46.595
the values that are
being excluded.

267
00:10:46.595 --> 00:10:48.830
Why you have missing
values really

268
00:10:48.830 --> 00:10:51.260
matters depending on the problem
you're trying to solve.

269
00:10:51.260 --> 00:10:53.030
It might be unreasonable to

270
00:10:53.030 --> 00:10:54.890
infer missing values
for instance,

271
00:10:54.890 --> 00:10:58.580
if the Data shouldn't
exist in the first place.